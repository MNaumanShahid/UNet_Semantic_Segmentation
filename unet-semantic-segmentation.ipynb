{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6927,"databundleVersionId":45059,"sourceType":"competition"},{"sourceId":3009448,"sourceType":"datasetVersion","datasetId":1843391}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.listdir(\"/kaggle/input/carvana-image-masking-png\")","metadata":{"execution":{"iopub.status.busy":"2024-03-01T19:04:25.376717Z","iopub.execute_input":"2024-03-01T19:04:25.377049Z","iopub.status.idle":"2024-03-01T19:04:25.396852Z","shell.execute_reply.started":"2024-03-01T19:04:25.377020Z","shell.execute_reply":"2024-03-01T19:04:25.395750Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"['train_images', 'train_masks']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport math\nimport glob\nimport os\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport torchvision.utils\nimport pytorch_lightning as pl\nimport torchmetrics as tm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom typing import List\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-03-01T19:04:29.825920Z","iopub.execute_input":"2024-03-01T19:04:29.826515Z","iopub.status.idle":"2024-03-01T19:04:40.656387Z","shell.execute_reply.started":"2024-03-01T19:04:29.826471Z","shell.execute_reply":"2024-03-01T19:04:40.655595Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\n\nimport torchvision.transforms.functional as TF\n\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNET(nn.Module):\n    def __init__(\n            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n                 ):\n        super(UNET, self).__init__()\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Down part of UNET\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n\n        # Up part of UNET\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(\n                    feature*2, feature, kernel_size=2, stride=2\n                )\n            )\n            self.ups.append(DoubleConv(feature*2, feature))\n\n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        skip_connections = []\n\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:])\n\n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)\n\n        return self.final_conv(x)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T19:04:40.658369Z","iopub.execute_input":"2024-03-01T19:04:40.659092Z","iopub.status.idle":"2024-03-01T19:04:40.674255Z","shell.execute_reply.started":"2024-03-01T19:04:40.659056Z","shell.execute_reply":"2024-03-01T19:04:40.673394Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class SegmentationDataset(torch.utils.data.Dataset):\n  def __init__(self, image_path, mask_path, transforms):\n    self.images = glob.glob(os.path.join(image_path, '*.jpg'))\n    self.image_path = image_path\n    self.mask_path = mask_path\n    self.transforms = transforms\n\n  def __len__(self):\n    return len(self.images)\n  \n  def __getitem__(self, idx):\n    img = np.array(Image.open(self.images[idx]).convert('RGB'))\n    mask = np.array(Image.open(os.path.join(self.mask_path, os.path.basename(self.images[idx]).replace('.jpg', '.png')))) \n    mask[mask == 255.0] = 1.0  \n    augmentations = self.transforms(image=img, mask=mask)\n    image = augmentations[\"image\"]\n    mask = augmentations[\"mask\"]\n    mask = torch.unsqueeze(mask, 0)\n    mask = mask.type(torch.float32)\n    return image, mask","metadata":{"execution":{"iopub.status.busy":"2024-03-01T19:04:40.675346Z","iopub.execute_input":"2024-03-01T19:04:40.675619Z","iopub.status.idle":"2024-03-01T19:04:40.687182Z","shell.execute_reply.started":"2024-03-01T19:04:40.675596Z","shell.execute_reply":"2024-03-01T19:04:40.686344Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"image_path = \"/kaggle/input/carvana-image-masking-png/train_images\"\nmask_path = \"/kaggle/input/carvana-image-masking-png/train_masks\"\ntransform = A.Compose(\n    [\n        A.Resize(height=360, width=480),\n        A.Rotate(limit=45, p=0.7),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.3),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,\n        ),\n        A.pytorch.ToTensorV2(),\n    ]\n)\nds = SegmentationDataset(image_path, mask_path, transform)\ntrain_size = math.floor(len(ds) * 0.9)\nval_size = len(ds) - train_size\ntrain_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\n\ntrain_loader = torch.utils.data.DataLoader(train_ds,\n                                              batch_size=16,\n                                              num_workers=2,\n                                              pin_memory=True,\n                                              shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_ds,\n                                            batch_size=16,\n                                            num_workers=2,\n                                            pin_memory=True,\n                                            shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T19:04:40.688926Z","iopub.execute_input":"2024-03-01T19:04:40.689239Z","iopub.status.idle":"2024-03-01T19:04:40.896219Z","shell.execute_reply.started":"2024-03-01T19:04:40.689213Z","shell.execute_reply":"2024-03-01T19:04:40.895216Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"os.mkdir(\"/kaggle/working/saved_images/\")","metadata":{"execution":{"iopub.status.busy":"2024-03-01T19:04:40.897392Z","iopub.execute_input":"2024-03-01T19:04:40.897688Z","iopub.status.idle":"2024-03-01T19:04:40.902136Z","shell.execute_reply.started":"2024-03-01T19:04:40.897662Z","shell.execute_reply":"2024-03-01T19:04:40.901075Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\n\ndef save_checkpoint(state, filename=\"/kaggle/working/my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\ndef load_checkpoint(checkpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    \ndef check_accuracy(loader, model, device=\"cuda\"):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n\n    with torch.inference_mode():\n        for X, y in loader:\n            X = X.to(device)\n            y = y.to(device)\n            preds = torch.sigmoid(model(X))\n            preds = (preds > 0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            dice_score += (2 * (preds * y).sum()) / (\n                (preds + y).sum() + 1e-8\n            )\n\n    print(\n        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n    )\n    print(f\"Dice score: {dice_score/len(loader)}\")\n    model.train()\n\ndef save_predictions_as_imgs(\n    loader, model, folder=\"/kaggle/working/saved_images/\", device=\"cuda\"\n):\n    model.eval()\n    for idx, (x, y) in enumerate(loader):\n        x = x.to(device=device)\n        with torch.inference_mode():\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n        torchvision.utils.save_image(\n            preds, f\"{folder}/pred_{idx}.png\"\n        )\n        torchvision.utils.save_image(y, f\"{folder}{idx}.png\")\n\n    model.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T19:04:40.903260Z","iopub.execute_input":"2024-03-01T19:04:40.903534Z","iopub.status.idle":"2024-03-01T19:04:40.915064Z","shell.execute_reply.started":"2024-03-01T19:04:40.903511Z","shell.execute_reply":"2024-03-01T19:04:40.914283Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.empty_cache()\n\nimport torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nLEARNING_RATE = 1e-4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nNUM_EPOCHS = 3\nNUM_WORKERS = 2\nIMAGE_HEIGHT = 160  # 1280 originally\nIMAGE_WIDTH = 240 \nLOAD_MODEL = False\n\ndef train_fn(loader, model, optimizer, loss_fn, scaler):\n    loop = tqdm(loader)\n\n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(DEVICE)\n        targets = targets.float().to(DEVICE)\n\n        with torch.cuda.amp.autocast():\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        loop.set_postfix(loss=loss.item())\n        \n        \nmodel = UNET(in_channels=3, out_channels=1).to(DEVICE)\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nif LOAD_MODEL:\n    load_checkpoint(torch.load(\"/kaggle/working/my_checkpoint.pth.tar\"), model)\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(NUM_EPOCHS):\n    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint)\n\n    check_accuracy(val_loader, model, device=DEVICE)\n    \n    save_path = f\"/kaggle/working/saved_images/epoch{epoch}/\"\n    os.mkdir(save_path)\n\n    save_predictions_as_imgs(\n        val_loader, model, folder=save_path, device=DEVICE\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-01T19:04:40.916109Z","iopub.execute_input":"2024-03-01T19:04:40.916365Z","iopub.status.idle":"2024-03-01T19:21:53.608483Z","shell.execute_reply.started":"2024-03-01T19:04:40.916340Z","shell.execute_reply":"2024-03-01T19:21:53.607266Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"  0%|          | 0/287 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n100%|██████████| 287/287 [04:44<00:00,  1.01it/s, loss=0.128]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 87156477/87955200 with acc 99.09\nDice score: 0.9788597822189331\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 287/287 [04:46<00:00,  1.00it/s, loss=0.0763]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 87285728/87955200 with acc 99.24\nDice score: 0.9819754958152771\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 287/287 [04:45<00:00,  1.00it/s, loss=0.0606]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 87286646/87955200 with acc 99.24\nDice score: 0.9822328686714172\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(4, 6):\n    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint)\n\n    check_accuracy(val_loader, model, device=DEVICE)\n    \n    save_path = f\"/kaggle/working/saved_images/epoch{epoch}/\"\n    os.mkdir(save_path)\n\n    save_predictions_as_imgs(\n        val_loader, model, folder=save_path, device=DEVICE\n    )","metadata":{"execution":{"iopub.status.busy":"2024-03-01T19:23:13.354902Z","iopub.execute_input":"2024-03-01T19:23:13.355327Z","iopub.status.idle":"2024-03-01T19:34:41.671272Z","shell.execute_reply.started":"2024-03-01T19:23:13.355295Z","shell.execute_reply":"2024-03-01T19:34:41.670134Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 287/287 [04:45<00:00,  1.00it/s, loss=0.0405]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 87470068/87955200 with acc 99.45\nDice score: 0.9869180917739868\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 287/287 [04:45<00:00,  1.01it/s, loss=0.0349]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 87533868/87955200 with acc 99.52\nDice score: 0.988730251789093\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}